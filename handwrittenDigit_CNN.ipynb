{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjDRwl4g9pIvwGB9pJo6fy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bibin001/Deep-Learning/blob/main/handwrittenDigit_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_LDzhmFFAXL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert MNIST data to tensors of 4D (#of images, height, width and colour channels)\n",
        "transform=transforms.ToTensor()"
      ],
      "metadata": {
        "id": "hg6MKGsnHfnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train data (MNIST data is available in torchvision dataset.)\n",
        "train_data=datasets.MNIST(root='/content/gdrive/My Drive/Colab Notebooks/Computer Vision/examples/cnn_mnist_data', train=True, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "RrIYASuqH6V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test data (MNIST data is available in torchvision dataset.)\n",
        "test_data=datasets.MNIST(root='/content/gdrive/My Drive/Colab Notebooks/Computer Vision/examples/cnn_mnist_data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "9Fm3_a1vJ-cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create small batch size for images.. say 10\n",
        "train_loader=DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "test_loader=DataLoader(test_data, batch_size=10, shuffle=False)"
      ],
      "metadata": {
        "id": "4EIrsqEVKR-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define CNN model\n",
        "#Describe convolution layes and whats it doing (2 convolution layer)\n",
        "#THIS IS JUST EXAMPLE for a single image... GO TO ### MODEL CLASS\n",
        "\n",
        "conv1 = nn.Conv2d(1,6,3,1)\n",
        "conv2 = nn.Conv2d(6,16,3,1)"
      ],
      "metadata": {
        "id": "aKKCyJ5bVqPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grab 1 MNIST record (image)\n",
        "\n",
        "for i, (X_train, y_train) in enumerate(train_data):\n",
        "  break"
      ],
      "metadata": {
        "id": "faEuxxFlWzmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of one image:', X_train.shape)\n",
        "x=X_train.reshape(1,1,28,28)  #first 1 is for batch.\n",
        "print('Shape of reshaped image:', x.shape)"
      ],
      "metadata": {
        "id": "kWYAZEM6XNe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform our first convolution\n",
        "x=F.relu(conv1(x)) #relu as activation function"
      ],
      "metadata": {
        "id": "7EZAtwvvXOoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape #1 is single image, 6 no of filters, 26,26 is the dimension"
      ],
      "metadata": {
        "id": "a-WWv_NkZ8xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pass through the pooling layer\n",
        "x = F.max_pool2d(x,2,2)"
      ],
      "metadata": {
        "id": "yE7m33o1Z9NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "z1sfclCmbskO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#do our second convolution\n",
        "x=F.relu(conv2(x))"
      ],
      "metadata": {
        "id": "iVSoSVl8bthG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "hzAkDhPjc8Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pass through pooling layer\n",
        "x= F.max_pool2d(x, 2, 2)"
      ],
      "metadata": {
        "id": "YYjSMIzcc9KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape #11/2=5.5 but we have to round down to 5 as we cannot invent data to round up\n",
        "\n",
        "##########EXAMPLE OF ONE IMAGE OPERATION OF CONV AND POOLING OVER TO GET SHAPES OF IMAGES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou6QEKkpdZ9B",
        "outputId": "c1a37983-1285-47b7-b226-763a85f3ebdd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL CLASS\n",
        "class convNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1=nn.Conv2d(1,6,3,1)\n",
        "    self.conv2=nn.Conv2d(6,16,3,1)\n",
        "    #Fully connected layer\n",
        "    self.fc1=nn.Linear(5*5*16, 120) ##(16,5,5) is the shape of image after last pooling function. see just above.\n",
        "    self.fc2=nn.Linear(120, 70)\n",
        "    self.fc3=nn.Linear(70, 10)\n",
        "\n",
        "  def forward(self,X):\n",
        "    X = F.relu(self.conv1(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "    X = F.relu(self.conv2(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "    #two convolution layer and two pooling completed\n",
        "    #Now Flatten X (Data)\n",
        "    X=X.view(-1,16*5*5) #16*5*5 dimension/shape of image after last pooling, -1 is used to vary the batch size\n",
        "\n",
        "    #Fully Conneceted Layers\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = self.fc3(X)\n",
        "\n",
        "    return F.log_softmax(X, dim=1)\n"
      ],
      "metadata": {
        "id": "lkTailwmda0Y"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create instance of our Model\n",
        "torch.manual_seed(41)\n",
        "model= convNet()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0QqJejOj7pO",
        "outputId": "a4d0b8cd-2c79-49d7-80c0-70887991af23"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "convNet(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=70, bias=True)\n",
              "  (fc3): Linear(in_features=70, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss function & Optimizer\n",
        "criterion= nn.CrossEntropyLoss()\n",
        "optimizer= torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "CauTjIYOkbp7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time=time.time()\n",
        "\n",
        "#Create variables to track things\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "#For loop for epochs\n",
        "for i in range(epochs):\n",
        "  trn_corrct = 0\n",
        "  tst_correct = 0\n",
        "\n",
        "  #Train\n",
        "  for b,(X_train, y_train) in enumerate(train_loader):\n",
        "    b+=1 #start our batches at 1\n",
        "    y_pred=model(X_train)\n",
        "    loss= criterion(y_pred,y_train)\n",
        "\n",
        "    predicted=torch.max(y_pred.data,1)[1] #ass up the number of correct predictions\n",
        "    batch_correct= (predicted == y_train).sum() #How many we got correct from this batch, True=1, False=0, sum it up\n",
        "    trn_corrct+= batch_correct #Keep track as we go along training\n",
        "\n",
        "\n",
        "    #update our parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #print our results\n",
        "    if b%600 ==0:\n",
        "      print(f'Epoch: {i}, Batch: {b}, Loss: {loss.item()}')\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  train_correct.append(trn_corrct)\n",
        "\n",
        "  #Test\n",
        "  with torch.no_grad():\n",
        "    for b,(X_test, y_test) in enumerate(test_loader):\n",
        "      y_val=model(X_test)\n",
        "\n",
        "      predicted=torch.max(y_val.data,1)[1] #ass up the number of correct predictions\n",
        "      tst_correct+= (predicted == y_test).sum() #How many we got correct from this batch, True=1, False=0, sum it up\n",
        "\n",
        "  loss=criterion(y_val, y_test)\n",
        "  test_losses.append(loss)\n",
        "  test_correct.append(tst_correct)\n",
        "\n",
        "\n",
        "\n",
        "current_time=time.time()\n",
        "total_time= current_time-start_time\n",
        "print(f'Training took {total_time/60} minutes!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C6rmFpMlVzn",
        "outputId": "2ab85af7-68ef-47ca-e526-ad4da109672e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Batch: 600, Loss: 0.3765566945075989\n",
            "Epoch: 0, Batch: 1200, Loss: 0.5164510011672974\n",
            "Epoch: 0, Batch: 1800, Loss: 0.018596014007925987\n",
            "Epoch: 0, Batch: 2400, Loss: 0.0073001692071557045\n",
            "Epoch: 0, Batch: 3000, Loss: 0.1986463963985443\n",
            "Epoch: 0, Batch: 3600, Loss: 0.10770054161548615\n",
            "Epoch: 0, Batch: 4200, Loss: 0.01655733585357666\n",
            "Epoch: 0, Batch: 4800, Loss: 0.039656832814216614\n",
            "Epoch: 0, Batch: 5400, Loss: 0.004006581846624613\n",
            "Epoch: 0, Batch: 6000, Loss: 0.0347854420542717\n",
            "Epoch: 1, Batch: 600, Loss: 0.003279051510617137\n",
            "Epoch: 1, Batch: 1200, Loss: 0.0008063906570896506\n",
            "Epoch: 1, Batch: 1800, Loss: 0.00830929446965456\n",
            "Epoch: 1, Batch: 2400, Loss: 0.07041387259960175\n",
            "Epoch: 1, Batch: 3000, Loss: 0.07094080746173859\n",
            "Epoch: 1, Batch: 3600, Loss: 0.17314305901527405\n",
            "Epoch: 1, Batch: 4200, Loss: 0.0005820575752295554\n",
            "Epoch: 1, Batch: 4800, Loss: 0.0006530469981953502\n",
            "Epoch: 1, Batch: 5400, Loss: 0.2820059657096863\n",
            "Epoch: 1, Batch: 6000, Loss: 0.0016632291954010725\n",
            "Epoch: 2, Batch: 600, Loss: 0.0009400321287102997\n",
            "Epoch: 2, Batch: 1200, Loss: 0.00040414967224933207\n",
            "Epoch: 2, Batch: 1800, Loss: 0.000615531753282994\n",
            "Epoch: 2, Batch: 2400, Loss: 0.0002193798718508333\n",
            "Epoch: 2, Batch: 3000, Loss: 0.0029091346077620983\n",
            "Epoch: 2, Batch: 3600, Loss: 0.0008422993123531342\n",
            "Epoch: 2, Batch: 4200, Loss: 0.00031592263258062303\n",
            "Epoch: 2, Batch: 4800, Loss: 0.000705899961758405\n",
            "Epoch: 2, Batch: 5400, Loss: 0.0006389297777786851\n",
            "Epoch: 2, Batch: 6000, Loss: 0.006748677231371403\n",
            "Epoch: 3, Batch: 600, Loss: 0.0006675210897810757\n",
            "Epoch: 3, Batch: 1200, Loss: 0.000831428449600935\n",
            "Epoch: 3, Batch: 1800, Loss: 0.009265844710171223\n",
            "Epoch: 3, Batch: 2400, Loss: 0.14019155502319336\n",
            "Epoch: 3, Batch: 3000, Loss: 0.000572186429053545\n",
            "Epoch: 3, Batch: 3600, Loss: 0.000663158658426255\n",
            "Epoch: 3, Batch: 4200, Loss: 0.0017208721255883574\n",
            "Epoch: 3, Batch: 4800, Loss: 0.0004212136846035719\n",
            "Epoch: 3, Batch: 5400, Loss: 0.12729820609092712\n",
            "Epoch: 3, Batch: 6000, Loss: 0.04390322417020798\n",
            "Epoch: 4, Batch: 600, Loss: 0.31243735551834106\n",
            "Epoch: 4, Batch: 1200, Loss: 9.994504216592759e-05\n",
            "Epoch: 4, Batch: 1800, Loss: 0.0003760636318475008\n",
            "Epoch: 4, Batch: 2400, Loss: 0.0002103277074638754\n",
            "Epoch: 4, Batch: 3000, Loss: 0.039188213646411896\n",
            "Epoch: 4, Batch: 3600, Loss: 0.571316123008728\n",
            "Epoch: 4, Batch: 4200, Loss: 0.6268907785415649\n",
            "Epoch: 4, Batch: 4800, Loss: 0.004077418241649866\n",
            "Epoch: 4, Batch: 5400, Loss: 4.1622621210990474e-05\n",
            "Epoch: 4, Batch: 6000, Loss: 0.02118460275232792\n",
            "Training took 3.839461076259613 minutes!\n"
          ]
        }
      ]
    }
  ]
}